{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Game Prioritization\n",
    "\n",
    "Score each game for potential suspiciousness to prioritize deep analysis.\n",
    "\n",
    "**Inputs:**\n",
    "- Phase 1: games.parquet, game_aggregates.parquet\n",
    "- Phase 2: quick_stats.json\n",
    "\n",
    "**Outputs:**\n",
    "- `priority_scores.parquet` - Games with suspicion scores\n",
    "- `high_priority_games.json` - List of games for deep analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (injected by Papermill)\n",
    "username = \"default_user\"  # Chess.com username\n",
    "min_suspicion_score = 3  # Minimum score for deep analysis\n",
    "max_games_to_analyze = 50  # Maximum games for deep analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nsys.path.insert(0, '..')\nfrom common import (\n    setup_notebook, validate_parameters, print_section, print_subsection,\n    get_user_data_dir, save_phase_output, load_phase_output,\n    load_dataset_parquet,\n)\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsetup_notebook()\nvalidate_parameters(username)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\nprint_section(f\"GAME PRIORITIZATION: {username}\")\n\nuser_data_dir = get_user_data_dir(username)\n\n# Load games with aggregates\ngames_df = pd.DataFrame(load_dataset_parquet(user_data_dir / \"games.parquet\"))\nprint(f\"Games loaded: {len(games_df)}\")\n\ntry:\n    aggregates_df = pd.DataFrame(load_dataset_parquet(user_data_dir / \"game_aggregates.parquet\"))\n    games_df = games_df.merge(aggregates_df, on='game_id', how='left', suffixes=('', '_agg'))\n    print(f\"Merged with aggregates: {len(aggregates_df)} rows\")\nexcept FileNotFoundError:\n    print(\"No aggregates found, using games only\")\n\n# Load quick stats\nquick_stats = load_phase_output(username, \"phase2\", \"quick_stats.json\")\navg_rating = quick_stats['elo_analysis'].get('current_rating', 1500)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define suspicion scoring function\n",
    "def calculate_game_suspicion(row):\n",
    "    \"\"\"\n",
    "    Calculate a suspicion score for a game.\n",
    "    \n",
    "    Higher scores indicate games that should be analyzed more closely.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Win against higher-rated opponent (+2)\n",
    "    rating_diff = row.get('player_rating', 1500) - row.get('opponent_rating', 1500)\n",
    "    if row.get('player_result') == 'win' and rating_diff < -100:\n",
    "        score += 2\n",
    "    \n",
    "    # Large material swings in won game (+1)\n",
    "    if row.get('player_result') == 'win':\n",
    "        if row.get('material_trajectory') == 'volatile':\n",
    "            score += 1\n",
    "        if row.get('max_material_behind', 0) > 300:\n",
    "            score += 1\n",
    "    \n",
    "    # High fragility positions (+1)\n",
    "    if row.get('fragility_category') == 'high':\n",
    "        score += 1\n",
    "    \n",
    "    # Suspicious time patterns (+1)\n",
    "    avg_move_time = row.get('avg_move_time', 10)\n",
    "    if avg_move_time < 2:\n",
    "        score += 1  # Very fast moves\n",
    "    \n",
    "    # Won by resignation in complex position (+1)\n",
    "    if row.get('termination') == 'resignation' and row.get('player_result') == 'win':\n",
    "        if row.get('game_length', 40) > 40:\n",
    "            score += 1\n",
    "    \n",
    "    # Game vs banned opponent (-1, less interesting for cheat detection)\n",
    "    if row.get('opponent_is_banned', False):\n",
    "        score -= 1\n",
    "    \n",
    "    return max(0, score)\n",
    "\n",
    "# Calculate suspicion scores\n",
    "games_df['suspicion_score'] = games_df.apply(calculate_game_suspicion, axis=1)\n",
    "\n",
    "print(f\"\\nSuspicion score distribution:\")\n",
    "print(games_df['suspicion_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and prioritize games\n",
    "priority_cols = [\n",
    "    'game_id', 'game_date', 'opponent_username',\n",
    "    'player_rating', 'opponent_rating', 'player_result',\n",
    "    'time_class', 'termination', 'game_length',\n",
    "    'suspicion_score'\n",
    "]\n",
    "\n",
    "# Select available columns\n",
    "available_cols = [c for c in priority_cols if c in games_df.columns]\n",
    "prioritized_df = games_df[available_cols].copy()\n",
    "prioritized_df = prioritized_df.sort_values('suspicion_score', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 games by suspicion score:\")\n",
    "display_cols = ['game_date', 'opponent_username', 'player_result', 'suspicion_score']\n",
    "display_cols = [c for c in display_cols if c in prioritized_df.columns]\n",
    "print(prioritized_df[display_cols].head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select high-priority games for deep analysis\nMIN_GAMES_FOR_ANALYSIS = 10  # Always try to analyze at least this many games\n\nhigh_priority = prioritized_df[\n    prioritized_df['suspicion_score'] >= min_suspicion_score\n].head(max_games_to_analyze)\n\nprint(f\"\\nGames selected for deep analysis:\")\nprint(f\"  Suspicion threshold: >= {min_suspicion_score}\")\nprint(f\"  Max games: {max_games_to_analyze}\")\nprint(f\"  High-suspicion games found: {len(high_priority)}\")\n\n# Ensure we have enough games for meaningful analysis\ntotal_games = len(prioritized_df)\ntarget_count = min(MIN_GAMES_FOR_ANALYSIS, total_games)  # Can't analyze more than we have\n\nif len(high_priority) < target_count:\n    print(f\"\\n  Only {len(high_priority)} high-suspicion games found.\")\n    print(f\"  Falling back to top {target_count} games by suspicion score...\")\n    \n    # Take top games by score (includes high-suspicion + next highest scores)\n    high_priority = prioritized_df.head(target_count)\n    \n    # Add note about fallback selection\n    fallback_count = target_count - len(prioritized_df[prioritized_df['suspicion_score'] >= min_suspicion_score])\n    if fallback_count > 0:\n        print(f\"  Added {fallback_count} additional games below threshold\")\n\nprint(f\"\\n  Final selection: {len(high_priority)} games\")\nif len(high_priority) > 0:\n    print(f\"  Score range: {high_priority['suspicion_score'].min()} - {high_priority['suspicion_score'].max()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Suspicion score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of score distribution\n",
    "score_counts = games_df['suspicion_score'].value_counts().sort_index()\n",
    "axes[0].bar(score_counts.index, score_counts.values, color='steelblue')\n",
    "axes[0].axvline(x=min_suspicion_score, color='red', linestyle='--', label=f'Threshold ({min_suspicion_score})')\n",
    "axes[0].set_xlabel('Suspicion Score')\n",
    "axes[0].set_ylabel('Number of Games')\n",
    "axes[0].set_title('Suspicion Score Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Score vs rating difference\n",
    "if 'player_rating' in games_df.columns and 'opponent_rating' in games_df.columns:\n",
    "    rating_diff = games_df['opponent_rating'] - games_df['player_rating']\n",
    "    colors = ['green' if r == 'win' else 'red' if r == 'loss' else 'gray' \n",
    "              for r in games_df.get('player_result', ['draw']*len(games_df))]\n",
    "    axes[1].scatter(rating_diff, games_df['suspicion_score'], c=colors, alpha=0.5)\n",
    "    axes[1].set_xlabel('Rating Difference (Opponent - Player)')\n",
    "    axes[1].set_ylabel('Suspicion Score')\n",
    "    axes[1].set_title('Suspicion vs Rating Difference')\n",
    "    axes[1].axhline(y=min_suspicion_score, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "save_phase_output(username, \"phase3\", \"priority_scores.parquet\", prioritized_df)\n",
    "\n",
    "high_priority_list = high_priority['game_id'].tolist()\n",
    "high_priority_data = {\n",
    "    \"username\": username,\n",
    "    \"min_suspicion_score\": min_suspicion_score,\n",
    "    \"max_games\": max_games_to_analyze,\n",
    "    \"selected_count\": len(high_priority_list),\n",
    "    \"game_ids\": high_priority_list,\n",
    "}\n",
    "save_phase_output(username, \"phase3\", \"high_priority_games.json\", high_priority_data)\n",
    "\n",
    "print(f\"\\nPhase 3 complete!\")\n",
    "print(f\"High priority games saved: {len(high_priority_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}