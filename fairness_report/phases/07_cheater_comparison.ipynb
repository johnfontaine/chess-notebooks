{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7: Cheater Baseline Comparison\n",
    "\n",
    "Compare player metrics against trusted and cheater baselines.\n",
    "\n",
    "**Inputs:**\n",
    "- All previous phase outputs\n",
    "- Baseline data (trusted and cheater)\n",
    "\n",
    "**Outputs:**\n",
    "- `cheater_comparison.json` - Comparison with baselines\n",
    "- `risk_assessment.json` - Overall risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (injected by Papermill)\n",
    "username = \"default_user\"  # Chess.com username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nsys.path.insert(0, '..')\nfrom common import (\n    setup_notebook, validate_parameters, print_section, print_subsection,\n    get_user_data_dir, save_phase_output, load_phase_output,\n    load_dataset_parquet, load_baseline,\n    BASELINE_DIR, CHEATER_DIR,\n)\nimport json\nimport pandas as pd\nimport numpy as np\n\nsetup_notebook()\nvalidate_parameters(username)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all analysis outputs\n",
    "print_section(f\"CHEATER COMPARISON: {username}\")\n",
    "\n",
    "# Load baselines\n",
    "trusted_baseline = load_baseline(\"trusted\")\n",
    "cheater_baseline = load_baseline(\"cheater\")\n",
    "\n",
    "print(f\"Trusted baseline: {trusted_baseline.get('num_players', 0)} players, {trusted_baseline.get('total_games', 0)} games\")\n",
    "print(f\"Cheater baseline: {cheater_baseline.get('num_players', 0)} players, {cheater_baseline.get('total_games', 0)} games\")\n",
    "\n",
    "# Load player analysis\n",
    "player_metrics = {}\n",
    "\n",
    "# Phase 2: Quick stats\n",
    "try:\n",
    "    quick_stats = load_phase_output(username, \"phase2\", \"quick_stats.json\")\n",
    "    player_metrics['elo'] = quick_stats.get('elo_analysis', {})\n",
    "    player_metrics['results'] = quick_stats.get('result_patterns', {})\n",
    "    print(\"Loaded Phase 2 quick stats\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 2 not available\")\n",
    "\n",
    "# Phase 4a: Engine analysis\n",
    "try:\n",
    "    engine_df = load_phase_output(username, \"phase4a\", \"engine_analysis.parquet\")\n",
    "    player_metrics['engine'] = {\n",
    "        'avg_acpl': engine_df['acpl'].mean(),\n",
    "        'avg_accuracy': engine_df['accuracy'].mean(),\n",
    "        'best_move_rate': engine_df['best_move_rate'].mean(),\n",
    "    }\n",
    "    print(f\"Loaded Phase 4a engine analysis ({len(engine_df)} games)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 4a not available\")\n",
    "\n",
    "# Phase 4b: Regan analysis\n",
    "try:\n",
    "    regan_df = load_phase_output(username, \"phase4b\", \"regan_analysis.parquet\")\n",
    "    player_metrics['regan'] = {\n",
    "        'avg_z_score': regan_df['z_score'].mean(),\n",
    "        'max_z_score': regan_df['z_score'].max(),\n",
    "        'flagged_games': int(regan_df['is_flagged'].sum()),\n",
    "        'move_match_rate': regan_df['move_match_rate'].mean(),\n",
    "    }\n",
    "    print(f\"Loaded Phase 4b Regan analysis ({len(regan_df)} games)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 4b not available\")\n",
    "\n",
    "# Phase 4c: Tablebase\n",
    "try:\n",
    "    tb_consistency = load_phase_output(username, \"phase4c\", \"tablebase_consistency.json\")\n",
    "    player_metrics['tablebase'] = {\n",
    "        'accuracy': tb_consistency.get('overall_accuracy', 0),\n",
    "        'perfect_rate': tb_consistency.get('perfect_games', 0) / max(1, tb_consistency.get('games_analyzed', 1)),\n",
    "    }\n",
    "    print(\"Loaded Phase 4c tablebase analysis\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 4c not available\")\n",
    "\n",
    "# Phase 5: Time analysis\n",
    "try:\n",
    "    time_analysis = load_phase_output(username, \"phase5\", \"time_analysis.json\")\n",
    "    player_metrics['time'] = {\n",
    "        'avg_move_time': time_analysis.get('avg_move_time', 0),\n",
    "        'instant_rate': time_analysis.get('instant_move_rate', 0),\n",
    "        'suspicious_games': time_analysis.get('suspicious_games', 0),\n",
    "    }\n",
    "    print(\"Loaded Phase 5 time analysis\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 5 not available\")\n",
    "\n",
    "# Phase 6: Maia2\n",
    "try:\n",
    "    maia_df = load_phase_output(username, \"phase6\", \"maia2_analysis.parquet\")\n",
    "    player_metrics['maia2'] = {\n",
    "        'avg_humanness': maia_df['humanness_score'].mean(),\n",
    "        'min_humanness': maia_df['humanness_score'].min(),\n",
    "    }\n",
    "    print(f\"Loaded Phase 6 Maia2 analysis ({len(maia_df)} games)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Phase 6 not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare to baselines\nprint_subsection(\"BASELINE COMPARISON\")\n\ncomparisons = []\n\n# Helper function\ndef compare_metric(name, player_val, trusted_mean, trusted_std, cheater_mean=None, higher_is_suspicious=True):\n    \"\"\"Compare player value to baseline.\"\"\"\n    if player_val is None or trusted_mean is None:\n        return None\n    \n    # Z-score relative to trusted baseline\n    z_vs_trusted = (player_val - trusted_mean) / max(trusted_std, 0.001)\n    \n    # Closer to cheater or trusted?\n    if cheater_mean is not None:\n        dist_to_trusted = abs(player_val - trusted_mean)\n        dist_to_cheater = abs(player_val - cheater_mean)\n        closer_to = 'cheater' if dist_to_cheater < dist_to_trusted else 'trusted'\n    else:\n        closer_to = 'unknown'\n    \n    # Is this suspicious?\n    if higher_is_suspicious:\n        suspicious = z_vs_trusted > 2\n    else:\n        suspicious = z_vs_trusted < -2\n    \n    return {\n        'metric': name,\n        'player_value': player_val,\n        'trusted_mean': trusted_mean,\n        'trusted_std': trusted_std,\n        'cheater_mean': cheater_mean,\n        'z_vs_trusted': z_vs_trusted,\n        'closer_to': closer_to,\n        'suspicious': suspicious,\n    }\n\n# Compare key metrics\nif 'elo' in player_metrics and trusted_baseline.get('elo_baseline'):\n    tb = trusted_baseline['elo_baseline']\n    cb = cheater_baseline.get('elo_baseline', {}) if cheater_baseline else {}\n    \n    comp = compare_metric(\n        'manipulation_score',\n        player_metrics['elo'].get('manipulation_score'),\n        tb.get('manipulation_score_mean'),\n        tb.get('manipulation_score_std', 0.1),\n        cb.get('manipulation_score_mean'),\n        higher_is_suspicious=True\n    )\n    if comp:\n        comparisons.append(comp)\n\nif 'engine' in player_metrics:\n    # Compare accuracy to cheater baseline if available\n    # Note: accuracy is on 0-100 scale (Lichess-style percentage)\n    comp = compare_metric(\n        'accuracy',\n        player_metrics['engine'].get('avg_accuracy'),\n        75,  # Typical human accuracy (0-100 scale)\n        10,  # Standard deviation\n        90,  # Typical cheater accuracy\n        higher_is_suspicious=True\n    )\n    if comp:\n        comparisons.append(comp)\n    \n    comp = compare_metric(\n        'best_move_rate',\n        player_metrics['engine'].get('best_move_rate'),\n        0.40,  # Typical human best move rate\n        0.10,\n        0.70,  # Typical cheater best move rate\n        higher_is_suspicious=True\n    )\n    if comp:\n        comparisons.append(comp)\n\nif 'regan' in player_metrics:\n    comp = compare_metric(\n        'z_score',\n        player_metrics['regan'].get('avg_z_score'),\n        0.0,  # Expected z-score for fair play\n        1.0,\n        2.5,  # Typical cheater z-score\n        higher_is_suspicious=True\n    )\n    if comp:\n        comparisons.append(comp)\n\nif 'time' in player_metrics:\n    comp = compare_metric(\n        'instant_rate',\n        player_metrics['time'].get('instant_rate'),\n        0.15,  # Typical human instant rate\n        0.10,\n        0.40,  # Typical bot instant rate\n        higher_is_suspicious=True\n    )\n    if comp:\n        comparisons.append(comp)\n\n# Display comparisons\nif comparisons:\n    comp_df = pd.DataFrame(comparisons)\n    print(comp_df[['metric', 'player_value', 'trusted_mean', 'z_vs_trusted', 'closer_to', 'suspicious']].to_string())\nelse:\n    print(\"No metrics available for comparison.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall risk score\n",
    "print_subsection(\"RISK ASSESSMENT\")\n",
    "\n",
    "risk_factors = []\n",
    "\n",
    "for comp in comparisons:\n",
    "    if comp['suspicious']:\n",
    "        risk_factors.append({\n",
    "            'metric': comp['metric'],\n",
    "            'z_score': comp['z_vs_trusted'],\n",
    "            'weight': 1.0\n",
    "        })\n",
    "\n",
    "# Add specific flags\n",
    "if 'regan' in player_metrics:\n",
    "    flagged = player_metrics['regan'].get('flagged_games', 0)\n",
    "    if flagged > 0:\n",
    "        risk_factors.append({\n",
    "            'metric': 'regan_flagged_games',\n",
    "            'z_score': flagged,\n",
    "            'weight': 2.0\n",
    "        })\n",
    "\n",
    "if 'time' in player_metrics:\n",
    "    suspicious_timing = player_metrics['time'].get('suspicious_games', 0)\n",
    "    if suspicious_timing > 5:\n",
    "        risk_factors.append({\n",
    "            'metric': 'suspicious_timing_games',\n",
    "            'z_score': suspicious_timing,\n",
    "            'weight': 1.5\n",
    "        })\n",
    "\n",
    "# Calculate weighted risk score\n",
    "if risk_factors:\n",
    "    total_weight = sum(rf['weight'] for rf in risk_factors)\n",
    "    weighted_sum = sum(rf['z_score'] * rf['weight'] for rf in risk_factors)\n",
    "    risk_score = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "else:\n",
    "    risk_score = 0\n",
    "\n",
    "# Determine risk level\n",
    "if risk_score < 1:\n",
    "    risk_level = \"LOW\"\n",
    "elif risk_score < 2:\n",
    "    risk_level = \"MODERATE\"\n",
    "elif risk_score < 3:\n",
    "    risk_level = \"HIGH\"\n",
    "else:\n",
    "    risk_level = \"VERY HIGH\"\n",
    "\n",
    "print(f\"\\nRisk Assessment:\")\n",
    "print(f\"  Risk Score: {risk_score:.2f}\")\n",
    "print(f\"  Risk Level: {risk_level}\")\n",
    "print(f\"  Risk Factors: {len(risk_factors)}\")\n",
    "\n",
    "if risk_factors:\n",
    "    print(f\"\\nRisk factors identified:\")\n",
    "    for rf in risk_factors:\n",
    "        print(f\"  - {rf['metric']}: z={rf['z_score']:.2f} (weight={rf['weight']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs\n",
    "comparison_output = {\n",
    "    \"username\": username,\n",
    "    \"player_metrics\": player_metrics,\n",
    "    \"comparisons\": comparisons,\n",
    "    \"baselines_used\": {\n",
    "        \"trusted_players\": trusted_baseline.get('num_players', 0),\n",
    "        \"trusted_games\": trusted_baseline.get('total_games', 0),\n",
    "        \"cheater_players\": cheater_baseline.get('num_players', 0) if cheater_baseline else 0,\n",
    "        \"cheater_games\": cheater_baseline.get('total_games', 0) if cheater_baseline else 0,\n",
    "    }\n",
    "}\n",
    "save_phase_output(username, \"phase7\", \"cheater_comparison.json\", comparison_output)\n",
    "\n",
    "risk_output = {\n",
    "    \"username\": username,\n",
    "    \"risk_score\": risk_score,\n",
    "    \"risk_level\": risk_level,\n",
    "    \"risk_factors\": risk_factors,\n",
    "    \"suspicious_metrics\": [c['metric'] for c in comparisons if c.get('suspicious')],\n",
    "}\n",
    "save_phase_output(username, \"phase7\", \"risk_assessment.json\", risk_output)\n",
    "\n",
    "print(f\"\\nPhase 7 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparisons:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metrics = [c['metric'] for c in comparisons]\n",
    "    z_scores = [c['z_vs_trusted'] for c in comparisons]\n",
    "    colors = ['red' if c['suspicious'] else 'green' for c in comparisons]\n",
    "    \n",
    "    bars = ax.barh(metrics, z_scores, color=colors)\n",
    "    ax.axvline(2, color='red', linestyle='--', alpha=0.5, label='Suspicious threshold')\n",
    "    ax.axvline(-2, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel('Z-Score vs Trusted Baseline')\n",
    "    ax.set_title(f'Player Comparison to Baseline: {username}')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}